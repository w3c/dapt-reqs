<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>DAPT Requirements</title>
    <script
      src="https://www.w3.org/Tools/respec/respec-w3c"
      class="remove"
      defer
    ></script>
    <script class="remove">
      // All config options at https://respec.org/docs/
      var respecConfig = {
        specStatus: "DNOTE",
        editors: [
            { name: "Cyril Concolato", w3cid: "36324", mailto: "cconcolato@netflix.com" },
            { name: "Nigel Megitt", w3cid: "64750", mailto: "nigel.megitt@bbc.co.uk"}
        ],
        github: {
            repoURL : "https://github.com/w3c/dapt-reqs/",
            branch : "main"
        },
        shortName: "dapt-reqs",
        noRecTrack: true,
        xref: "web-platform",
        group: "wg/timed-text",
        subjectPrefix: "[dapt-reqs]",
        localBiblio: {
            "ADPT": {
                publisher: "World Wide Web Consortium (W3C)",
                href:"https://w3c.github.io/adpt/",
                title: "Proposal for an Exchange Format to support Audio Description"
            },
            "TTAL": {
                publisher: "Netflix",
                href: "https://netflixtechblog.com/introducing-netflix-timed-text-authoring-lineage-6fb57b72ad41",
                title: "Introducing Netflix Timed Text Authoring Lineage"
            }
        }
      };
    </script>
    <style>
        table.coldividers td + td { border-left:1px solid gray; text-align: center;}
        table.syntax { border: 0px solid black; width: 85%; border-collapse: collapse }
        table.syntax caption { font-weight: bold; text-align: left; padding-bottom: 0.5em }
        table.syntax th { border: 0px solid black; text-align: left }
        table.syntax td { border: 0px solid black }
        table.syntax div { background-color: #ffffc8 }

        /*for the SVG - navigation highlighting */

        :focus path,
        :focus polygon,
        :focus ellipse,
        :focus rect {
            stroke-width: 2px;
        }
        g g g:hover a path,
        g g g:hover a polygon,
        g g g:hover a ellipse,
        g g g:hover a rect {
            stroke-width: 2px;
        }
        :focus text,
        g g g:hover a text,
        g g g:hover text tspan a {
            stroke: black;
            stroke-width: .5px;
        }
      </style>
    </head>
  <body>
    <section id="abstract">
      <p>This document captures technical requirements for a profile of TTML2 for use
          in workflows related to dubbing and audio description of movies and videos.</p>
    </section>
    <section id="sotd">
      <p>This is required.</p>
    </section>
    <section class="informative">
      <h2>Introduction</h2>
      <p>W3C members have identified the need for a profile of TTML for the exchange of timed text content
          in the context of audio description or dubbing applications.
          This document is based on the work of the
          <a href="https://www.w3.org/community/audio-description/">Audio Description Community Group</a>
          including [[ADPT]] and [[TTAL]].</p>
    </section>
    <section>
        <h2>Workflow</h2>
        <p>The following diagram illustrates a hypothetical combined workflow
          for the authoring and exchange of timed text content
          for each of
          1) audio description and
          2) dubbing applications.</p>
        <p>The contents of the diagram are fully described in the paragraphs that follow: it is provided as a visual summary only.</p>
        <p>The blue rectangles with labels preceded by a number represent various manual or automated processes.
           Clicking on a process rectangle will take the reader to a detailed description of the process from the table below.
           Processes specific to Audio Description or Dubbing workflows are marked respectively with AD or DUB.
           Those not marked as AD or DUB are common to both workflows.
           The green boxes with a wave underside represent timed text scripts produced by the various processes
           and expected to be compliant to the profile definition.
           The white boxes with a wave underside are media data used or produced in the workflows
           but whose format is out of scope for the profile.</p>

        <figure id='fig-workflow'>
            <div data-include="figures/workflow-fig.svg"></div>
            
            <figcaption>
                Hypothetical combined workflow for audio description and dubbing.
            </figcaption>
        </figure>
        <section>
          <h3>Identify key times in programme dialog, mark gaps: <dfn data-lt="PS1">Process Step 1</dfn>. </h3>
          <p>This step is in both the Audio Description and Dubbing workflows.</p>
          <p>The data output of this step is a set of <a>events</a>.</p>
          <p>In this step, the programme audio track is automatically or manually processed
            to identify intervals either with spoken dialogs
            to be translated or within which description audio may be inserted.</p>
          <p>This is the first process step.</p>
          <p>The next step is <a>PS2a</a> for audio description or <a>PS2b</a> for dubbing.</p>
        </section>
        <section>
          <h3>Describe images (AD): <dfn data-lt="PS2a">Process Step 2a</dfn>. </h3>
          <p>This step is in the Audio Description workflow only.</p>
          <p>The data input to this step is a set of timed <a>events</a>.</p>
          <p>The data output of this step is a pre-recording timed text script corresponding to the descriptions.</p>
          <p>In this step, a set of descriptions are written to fit within the identified times.</p>
          <p>The previous step is <a>PS1</a>.</p>
          <p>The next step is <a>PS3</a>.</p>
        </section>
        <section>
          <h3>Transcribe original text (DUB): <dfn data-lt="PS2b">Process Step 2b</dfn>. </h3>
          <p>This step is in the Dubbing workflow only.</p>
          <p>The data input to this step is a set of timed <a>events</a>.</p>
          <p>The data output of this step is timed text corresponding to the original language.</p>
          <p>In this step, the following actions are performed:</p>
          <ul>
            <li>Dialogue is transcribed in the original language to create a source transcription text.</li>
            <li>The dialogue events are notated with character information and other annotations.</li>
            <li>Localization notes are generated to guide further adaptation.</li>
          </ul>
          <p>The previous step is <a>PS1</a>.</p>
          <p>The next step is <a>PS2c</a>.</p>
        </section>
        <section>
          <h3>Translate text (DUB): <dfn data-lt="PS2c">Process Step 2c</dfn>. </h3>
          <p>This step is in the Dubbing workflow only.</p>
          <p>The data input to this step is timed text in the original language,
            including character information, annotations and localization notes.</p>
          <p>The data output of this step is timed text corresponding to both the original language
            and the dubbing (translation) language.</p>
          <p>In this step, the dialogue is translated to a target language.</p>
          <p>The previous step is <a>PS2b</a>.</p>
          <p>The next step is <a>PS2d</a>.</p>
        </section>
        <section>
          <h3>Adapt text (DUB): <dfn data-lt="PS2d">Process Step 2d</dfn>. </h3>
          <p>This step is in the Dubbing workflow only.</p>
          <p>The data input to this step is timed text in both the original language,
            including character information, annotations and localization notes, and
            the translated dialogue in the dubbing (translation) language.</p>
          <p>The data output of this step is a pre-recording timed text script
            corresponding to both the original language
            and the dubbing (translation) language adapted as needed for recording.</p>
          <p>In this step, the translation is adapted for dubbing;
            for example matching the actor's lip movements
            and considering reading speeds and shot changes.</p>
          <p>The previous step is <a>PS2c</a>.</p>
          <p>The next step is <a>PS3</a>.</p>
        </section>
        <section>
          <h3>Voice recording or synthesize audio: <dfn data-lt="PS3">Process Step 3</dfn>. </h3>
          <p>This step is in both the Audio Description and Dubbing workflows.</p>
          <p>The data input to this step is a pre-recording timed text script.</p>
          <p>The data output of this step is an audio rendering of the script.</p>
          <p>In this step, an audio rendition of the script is generated
            either by using an actor or voicer and recording the speech
            or by synthesizing the audio description by using a text to speech system.
            For audio descriptions, this is typically a mono audio track
            that may be delivered as a single track that is the same duration as the programme
            or as a set of short audio tracks each beginning at a defined time.
            Alternatively it can be a single track with each short description concatenated
            to remove unnecessary silences.</p>
          <p>The previous steps are <a>PS2a</a> or <a>PS2d</a>.</p>
          <p>The next steps are <a>PS4</a> and/or <a>PS5</a>.</p>
        </section>
        <section>
          <h3>Define audio mixing instructions (AD): <dfn data-lt="PS4">Process Step 4</dfn>. </h3>
          <p>This step is in the Audio Description workflow only.</p>
          <p>The data inputs to this step are the pre-recording timed text script and the rendered audio.</p>
          <p>The data output of this step is post-recording script including audio mixing instructions.</p>
          <p>In this step, the following actions are performed:</p>
          <ul>
            <li>Select a horizontal pan position to apply to the audio rendition
                of the description when mixing with the main programme audio.
                This is typically a single value that applies to each description.</li>
            <li>Select the amount by which to lower the main programme audio
                prior to mixing in the description audio.
                This is typically defined as a curve defined by a set of moments in time
                and fade levels to apply,
                with an interpolation algorithm to vary the levels between each moment in time.</li>
          </ul>
          <p>The previous step is <a>PS3</a>.</p>
          <p>This step can be followed by, or be done in parallel with, <a>PS5</a>, or it can be the final step.</p>
        </section>
        <section>
          <h3>Edit script to match performance: <dfn data-lt="PS5">Process Step 5</dfn>. </h3>
          <p>This step is in both the Audio Description and Dubbing workflows.</p>
          <p>The data inputs to this step are the pre-recording timed text script and the rendered audio.</p>
          <p>The data output of this step is an accurate post-recording script matching the recorded audio,
            sometimes called an <dfn>As-recorded script</dfn>.</p>
          <p>In this step, the script is adjusted based on changes made during the audio recording.</p>
          <p>The previous step is <a>PS3</a>.</p>
          <p>This step can be followed by, or be done in parallel with, <a>PS4</a>, or it can be the final step.</p>
        </section>

      </section>
    <section>
        <h2>Requirements</h2>
        <p>The following table lists the requirements at each stage of the workflow:</p>
        <p class="ednote">TODO: Add [[media-accessibility-reqs]]</p>
        <table class="simple">
            <caption>Table of requirements, each having a unique requirement number,
              links to the workflow process stages from which it arises,
              and a detailed description.</caption>
            <thead>
                <tr>
                    <th scope="col">Requirement number</th>
                    <th scope="col">Process step</th>
                    <th scope="col">Requirement</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>R1</td>
                    <td>all</td>
                    <td><p>A document shall identify the type of document it corresponds to,
                        from typical dubbing and audio description workflows,
                        at least among:
                        &quot;Audio Description Script&quot;,
                        &quot;Original Language Dialogue List&quot;,
                        &quot;Translated Dialogue List&quot; (a.k.a. &quot;Pivot Language Dialogue List&quot;),
                        &quot;Pre-Recording Dub Script&quot;,
                        &quot;Pre-Recording Audio Description Script&quot;,
                        &quot;As-recorded Dub Script&quot;,
                        &quot;As-recorded Audio Description Script&quot;.</p></td>
                    </tr>
                <tr>
                    <td>R2</td>
                    <td><a>PS1</a></td>
                    <td><p>A document must be able to define a list of intervals,
                        each called <dfn>event</dfn>,
                        defined by a begin time and an end time,
                        either matching a dialogue spoken by a single character or matching on-screen text from the programme,
                        or that is an opportunity for adding descriptions.</p>
                        <p class="ednote">In ADPT, the requirement for intervals was distinct from the requirements their contents - any reason not to do the same here?</p>
                        <p class="ednote">Is "event" the best name? Do nested events need a different term?</p>
                    </td>
                </tr>
                <tr>
                    <td>R3</td>
                    <td><a>PS2a</a></td>
                    <td><p>A document must be able to incorporate description text to be voiced,
                        each description located within a timed interval defined by a begin time and an end time.</p></td>
                </tr>
                <tr>
                    <td>R4</td>
                    <td><a>PS2a</a></td>
                    <td><p>A document must be able to incorporate additional user defined metadata
                        associated with each description;
                        metadata schemes may be user defined or centrally defined.
                        For example the language of the description may be stored,
                        or notes made by the script writer.</p></td>
                    </tr>
                <tr>
                    <td>R5</td>
                    <td><a>PS2a</a></td>
                    <td><p>A document must be extensible to allow incorporation of data required
                        to achieve the desired quality of audio presentation,
                        whether manual or automated.
                        For example it is typical to include information about
                        what gender and age voice would be appropriate to voice the descriptions;
                        it is also feasible to include data used to improve the quality of text to speech synthesis,
                        such as phonetic descriptions of the text, intonation and emotion data etc.
                        The format of any extensions for this purpose need not be defined.</p></td>
                </tr>
                <tr>
                    <td>R6</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able to describe the characters participating in a programme,
                        defined by a name in the programme,
                        optionally a name in the real world
                        and optional private metadata (e.g. images, text).</p></td>
                </tr>
                <tr>
                    <td>R7</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able to associate a unique character with a given <a>event</a>,
                        when the event corresponds to a dialogue.</p></td>
                </tr>
                <tr>
                    <td>R8</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able to associate text content,
                        called <dfn>original content</dfn>,
                        with each <a>event</a>,
                        corresponding to the content of the dialogue or on-screen text.
                        The language shall be identified too.</p></td>
                </tr>
                <tr>
                    <td>R9</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able to associate optional rendering styles with a character.</p></td>
                </tr>
                <tr>
                    <td>R10</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document shall support associating rendering styles with an <a>event</a>.</p></td>
                </tr>
                <tr>
                    <td>R11</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able to optionally associate a human-readable string with an <a>event</a>,
                        providing annotation of the event.</p></td>
                    </tr>
                <tr>
                    <td>R12</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document must be able, for an <a>event</a> corresponding to a dialogue,
                        to optionally indicate if the character is in the picture,
                        out of the picture or transitioning in or out of the picture.</p></td>
                </tr>
                <tr>
                    <td>R13</td>
                    <td><a>PS2b</a></td>
                    <td><p>A document shall support optionally annotating each <a>event</a>
                        with an indication of the type of event
                        (e.g. when the event corresponds to on-screen text,
                        if the text corresponds to &quot;Credits&quot; or &quot;Title&quot;;
                        or if the dialogue text is audible or not),
                        and optionally additional human readable text.</p></td>
                </tr>
                <tr>
                    <td>R14</td>
                    <td><a>PS2c</a></td>
                    <td><p>A document must be able to optionally associate with an <a>event</a>
                        a translation of the <a>original content</a> in a different, identified language.</p></td>
                </tr>
                <tr>
                    <td>R15</td>
                    <td><a>PS3</a></td>
                    <td><p>A document must be able to reference audio tracks either
                        included as binary data within the document
                        or separately.</p></td>
                </tr>
                <tr>
                    <td>R16</td>
                    <td><a>PS3</a></td>
                    <td><p>A document must be able to associate a begin time
                        with the beginning of playback of each audio track,
                        for the case that multiple audio tracks are created,
                        one per description.</p></td>
                </tr>
                <tr>
                    <td>R17</td>
                    <td><a>PS3</a></td>
                    <td><p>A document must be able to associate a begin time with a playback entry time
                        within an audio track,
                        for the case that a single audio track is generated
                        that contains multiple segments of audio to be played back at different times.</p></td>
                </tr>
                <tr>
                    <td>R18</td>
                    <td><a>PS4</a></td>
                    <td><p>A document must be able to associate a left/right pan value
                        with playback of each or every audio description.
                        This value applies to the audio description prior to mixing with the main programme audio.</p></td>
                </tr>
                <tr>
                    <td>R19</td>
                    <td><a>PS4</a></td>
                    <td><p>A document must be able to define a fade level curve
                        that applies to the main programme audio prior to mixing with the audio description,
                        where that fade level curve is defined by
                        a set of pairs of level and times
                        and an interpolation algorithm.</p></td>
                </tr>
                <tr>
                    <td>R20</td>
                    <td>all</td>
                    <td><p>A document shall support storing private metadata associated with the document,
                        such as a title,
                        an episode identifier,
                        a season identifier, etc.</p></td>
                </tr>
            </tbody>
        </table>
    </section>
  </body>
</html>
